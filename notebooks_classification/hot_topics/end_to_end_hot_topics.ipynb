{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jonas\n",
      "[nltk_data]     Nothnagel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Jonas\n",
      "[nltk_data]     Nothnagel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jonas\n",
      "[nltk_data]     Nothnagel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Jonas\n",
      "[nltk_data]     Nothnagel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''basics'''\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('../..', 'src')))\n",
    "sys.setrecursionlimit(20500)\n",
    "import vectorize_embed as em\n",
    "import make_dataset as mk\n",
    "import clean_dataset as clean\n",
    "import visualize as vis\n",
    "import pandas as pd\n",
    "import pickle \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tools as tools\n",
    "\n",
    "\n",
    "'''Plotting'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "'''features'''\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "'''Classifiers'''\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "'''Metrics/Evaluation'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn_hierarchical_classification.classifier import HierarchicalClassifier\n",
    "from sklearn_hierarchical_classification.constants import ROOT\n",
    "from sklearn_hierarchical_classification.metrics import h_fbeta_score, multi_labeled\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import operator    \n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "       'food_and_agricultural_commodities_strategy', 'green_recovery',\n",
    "       'health', 'human_rights', 'leaving_no_one_behind',\n",
    "       'multi_stakeholder_collaboration', 'nature_based_solution',\n",
    "       'plastic', 'poverty_reduction', 'public_private_partnership', 'sids',\n",
    "       'south_south_cooperation', 'structural_system_transformation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"function that pre-processes untext data into right format\"\"\"\n",
    "\n",
    "def predict_text(input_string, category, spacy = False, basic_clean = True, tfidf = False, context_emb = False, transf = False):\n",
    "\n",
    "    print('______')\n",
    "    input_list = [input_string]\n",
    "    input_df = pd.DataFrame(input_list, columns =['input_text'])\n",
    "    \n",
    "    \"\"\"input text will be normalise to the standard of training data:\"\"\"\n",
    "    if basic_clean == True:       \n",
    "        input_df['input_text'] = input_df['input_text'].apply(clean.basic)\n",
    "        clean_df = pd.Series(input_df['input_text'])\n",
    "    else:\n",
    "        pass\n",
    "    if spacy == True:\n",
    "        input_df['input_text'] = input_df['input_text'].apply(clean.spacy_clean)\n",
    "        clean_df = pd.Series(input_df['input_text'])\n",
    "    else:\n",
    "        pass\n",
    "      \n",
    "    \"\"\"input text will be vectorised/embedded:\n",
    "        \n",
    "        tf-idf\n",
    "    \"\"\"\n",
    "    if tfidf == True:\n",
    "        \n",
    "        \"\"\"load vectorizer and LSA dimension reducer\"\"\"\n",
    "        \n",
    "        tfidf_vectorizer = joblib.load('../../models/tf_idf/hot_topics/'+category+'_'+'vectorizer.sav')        \n",
    "        lsa = joblib.load('../../models/tf_idf/hot_topics/'+category+'_'+'lsa.sav')\n",
    "        \n",
    "        vector_df = tfidf_vectorizer.transform(clean_df)\n",
    "        vector_df = lsa.transform(vector_df)\n",
    "        \n",
    "        \"\"\"load models:\"\"\"\n",
    "        clf = joblib.load('../../models/tf_idf/hot_topics/'+category+'_'+'model.sav')\n",
    "        \n",
    "        \"\"\"predict\"\"\"\n",
    "        y_hat = clf.predict(vector_df)\n",
    "        y_prob = clf.predict_proba(vector_df)\n",
    "        \n",
    "        if y_hat == 1:\n",
    "            print(category)\n",
    "            print(y_hat)\n",
    "            print(\"YES. Confidence:\", y_prob[0][1].round(2)*100, \"%\")\n",
    "            hat.append(y_prob[0][1].round(2)*100)\n",
    "        if y_hat == 0:\n",
    "            print(category)\n",
    "            print(y_hat)\n",
    "            print(\"NO Confidence:\", y_prob[0][0].round(2)*100, \"%\")\n",
    "            hat.append(y_prob[0][0].round(2)*100)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \"\"\"\n",
    "        Roberta embeddings and SGD classifier:\n",
    "    \"\"\"\n",
    "    if context_emb == True:\n",
    "        \n",
    "        X = clean_df.tolist()\n",
    "        \n",
    "        \"\"\"load models:\"\"\"\n",
    "        clf = joblib.load('../../models/contextual_emb/hot_topics/'+category+'_'+'Roberta_'+'model.sav')\n",
    "        \n",
    "        \"\"\"embed and predict\"\"\"\n",
    "        #vector_df = em.get_embeddings(\"roberta-base-nli-stsb-mean-tokens\", clean_df)\n",
    "        \n",
    "        \"\"\"predict\"\"\"\n",
    "        y_hat = clf.predict(em.get_embeddings(\"roberta-base-nli-stsb-mean-tokens\", X))\n",
    "        y_prob = clf.predict_proba(em.get_embeddings(\"roberta-base-nli-stsb-mean-tokens\", X))\n",
    "        print(y_hat)\n",
    "        print(y_prob)\n",
    "        if y_hat == [1]:\n",
    "            print(category)\n",
    "            print(\"YES. Confidence:\", y_prob[0][1].round(3)*100, \"%\")\n",
    "            hat.append(y_prob[0][1].round(2)*100)\n",
    "        if y_hat == [0]:\n",
    "            print(category)\n",
    "            print(\"NO Confidence:\", y_prob[0][0].round(3)*100, \"%\")   \n",
    "            hat.append(y_prob[0][0].round(2)*100)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    \"\"\"\n",
    "        Transformer\n",
    "    \"\"\"\n",
    "    if transf == True:\n",
    "        \"\"\"predict with transformer\n",
    "        \n",
    "        needs pytorch and simpletransformer library\"\"\"\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print('______')\n",
    "    return y_hat, y_prob, hat    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your text: hi\n",
      "______\n",
      "food_and_agricultural_commodities_strategy\n",
      "[0]\n",
      "NO Confidence: 56.99999999999999 %\n",
      "______\n",
      "______\n",
      "green_recovery\n",
      "[0]\n",
      "NO Confidence: 98.0 %\n",
      "______\n",
      "______\n",
      "health\n",
      "[0]\n",
      "NO Confidence: 100.0 %\n",
      "______\n",
      "______\n",
      "human_rights\n",
      "[0]\n",
      "NO Confidence: 100.0 %\n",
      "______\n",
      "______\n",
      "leaving_no_one_behind\n",
      "[0]\n",
      "NO Confidence: 99.0 %\n",
      "______\n",
      "______\n",
      "multi_stakeholder_collaboration\n",
      "[0]\n",
      "NO Confidence: 54.0 %\n",
      "______\n",
      "______\n",
      "nature_based_solution\n",
      "[0]\n",
      "NO Confidence: 62.0 %\n",
      "______\n",
      "______\n",
      "plastic\n",
      "[1]\n",
      "YES. Confidence: 100.0 %\n",
      "______\n",
      "______\n",
      "poverty_reduction\n",
      "[0]\n",
      "NO Confidence: 63.0 %\n",
      "______\n",
      "______\n",
      "public_private_partnership\n",
      "[1]\n",
      "YES. Confidence: 99.0 %\n",
      "______\n",
      "______\n",
      "sids\n",
      "[0]\n",
      "NO Confidence: 65.0 %\n",
      "______\n",
      "______\n",
      "south_south_cooperation\n",
      "[0]\n",
      "NO Confidence: 80.0 %\n",
      "______\n",
      "______\n",
      "structural_system_transformation\n",
      "[0]\n",
      "NO Confidence: 80.0 %\n",
      "______\n"
     ]
    }
   ],
   "source": [
    "input_string =  input(\"Enter your text: \")\n",
    "hat = []\n",
    "for category in categories: \n",
    "    prediction, probability, hat = predict_text(input_string, category, tfidf =True, context_emb = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'food_and_agricultural_commodities_strategy': 56.99999999999999,\n",
       " 'green_recovery': 98.0,\n",
       " 'health': 100.0,\n",
       " 'human_rights': 100.0,\n",
       " 'leaving_no_one_behind': 99.0,\n",
       " 'multi_stakeholder_collaboration': 54.0,\n",
       " 'nature_based_solution': 62.0,\n",
       " 'plastic': 100.0,\n",
       " 'poverty_reduction': 63.0,\n",
       " 'public_private_partnership': 99.0,\n",
       " 'sids': 65.0,\n",
       " 'south_south_cooperation': 80.0,\n",
       " 'structural_system_transformation': 80.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dict(zip(categories, hat)) \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
