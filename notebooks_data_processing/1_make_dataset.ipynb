{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import pickle\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jonas\n",
      "[nltk_data]     Nothnagel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Jonas\n",
      "[nltk_data]     Nothnagel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "'''helper functions'''\n",
    "import make_dataset as mk # make dataset - data processing\n",
    "\n",
    "import make_dataset_PIMS_API as mkp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import API data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uses neural translation for french and spanish logframes. Takes approx 4h withoug GPU support.\n",
    "#df_api, df_logs = mk.import_api_data(all_timelines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process taxonomy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data shape: (655, 193)\n",
      "only rows that have valid PIMS ID: (655, 193)\n",
      "only columns with entries: (655, 192)\n",
      "\n",
      "\n",
      "appending indicators...\n",
      "done!\n",
      "______________________________\n",
      "\n",
      "extracting and merging all text fields - descriptions, objectives, outcomes, outputs, indicators...\n",
      "done!\n",
      "______________________________\n",
      "\n",
      "(655, 201)\n",
      "keep only projects that are currently under implementation...\n",
      "(0, 201)\n",
      "done!\n",
      "______________________________\n",
      "\n",
      "Pims_plus API is considered to complete training data....\n",
      "Remove duplications from different time lines - delete exact otherwise keep latest...\n",
      "done!\n",
      "shape of data after removing timeline duplications:\n",
      "0\n",
      "______________________________\n",
      "\n",
      "flag projects that were tagged by looking at project document...\n",
      "number of documents with and without proper text and only titles both          0\n",
      "right_only    0\n",
      "left_only     0\n",
      "Name: title_only, dtype: int64\n",
      "done!\n",
      "______________________________\n",
      "\n",
      "Checking list of PIMS_ID that are not implemented or cancelled and remove from dataframa if no text exists...\n",
      "Number of projects that will be removed: 0\n",
      "______________________________\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['title_y' 'all_text_and_title' 'all_text_and_title_y' 'output_1.1'\\n 'output_1.2' 'output_1.3' 'output_1.4' 'output_1.5' 'outcome_2'\\n 'output_2.1' 'output_2.2' 'output_2.3' 'output_2.4' 'output_2.5'\\n 'outcome_3' 'output_3.1' 'output_3.2' 'output_3.3' 'output_3.4'\\n 'output_3.5' 'outcome_4' 'output_4.1' 'output_4.2' 'output_4.3'\\n 'output_4.4' 'output_4.5' 'outcome_5' 'output_5.1' 'output_5.2'\\n 'output_5.3' 'output_5.4_(no_entry)' 'output_5.5_(no_entry)'\\n 'outcome_6_(no_entry)' 'output_6.1' 'output_6.2' 'outcome_1' 'rta'\\n 'project_objective' 'project_objective_2' 'project_description' 'taggers'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1249915c0cdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# create datasets: processed data, dataframe with only raw texts and download data from PIMS API to compare and fill in:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtaxonomy_processed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_training_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtaxonomy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompare_with_API\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munder_implementation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace_short\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'save data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\SDG AI LAB\\NCE\\projects\\NCE_Document_Classification\\src\\make_dataset.py\u001b[0m in \u001b[0;36mcreate_training_texts\u001b[1;34m(dataframe, compare_with_API, under_implementation, add_indicators, replace_short)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \"\"\"\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m     dataframe = dataframe.drop(columns=['title_y', 'all_text_and_title', 'all_text_and_title_y', 'output_1.1', 'output_1.2', 'output_1.3',\n\u001b[0m\u001b[0;32m    339\u001b[0m                             \u001b[1;34m'output_1.4'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'output_1.5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'outcome_2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'output_2.1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'output_2.2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m                             \u001b[1;34m'output_2.3'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'output_2.4'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'output_2.5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'outcome_3'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'output_3.1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jonas\\Anaconda3\\envs\\nce_1\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4161\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4162\u001b[0m         \"\"\"\n\u001b[1;32m-> 4163\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4164\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4165\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jonas\\Anaconda3\\envs\\nce_1\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3885\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3887\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3889\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jonas\\Anaconda3\\envs\\nce_1\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3938\u001b[0m                 \u001b[0mlabels_missing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3939\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raise\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlabels_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3940\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3942\u001b[0m             \u001b[0mslicer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['title_y' 'all_text_and_title' 'all_text_and_title_y' 'output_1.1'\\n 'output_1.2' 'output_1.3' 'output_1.4' 'output_1.5' 'outcome_2'\\n 'output_2.1' 'output_2.2' 'output_2.3' 'output_2.4' 'output_2.5'\\n 'outcome_3' 'output_3.1' 'output_3.2' 'output_3.3' 'output_3.4'\\n 'output_3.5' 'outcome_4' 'output_4.1' 'output_4.2' 'output_4.3'\\n 'output_4.4' 'output_4.5' 'outcome_5' 'output_5.1' 'output_5.2'\\n 'output_5.3' 'output_5.4_(no_entry)' 'output_5.5_(no_entry)'\\n 'outcome_6_(no_entry)' 'output_6.1' 'output_6.2' 'outcome_1' 'rta'\\n 'project_objective' 'project_objective_2' 'project_description' 'taggers'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# use helper functios from make_dataset to import tagging table:\n",
    "taxonomy = mk.import_raw_data('tagging_table_v17') \n",
    "print('')\n",
    "\n",
    "# use helper function to import and process indicators:\n",
    "indicators = mk.import_indicators()\n",
    "print('')\n",
    "\n",
    "# create datasets: processed data, dataframe with only raw texts and download data from PIMS API to compare and fill in:\n",
    "taxonomy_processed, pims = mk.create_training_texts(taxonomy, compare_with_API = True, under_implementation = True, replace_short = True)\n",
    "\n",
    "print('save data')\n",
    "taxonomy_processed.to_excel(os.path.abspath(os.path.join('..', 'data/interim'))+'/taxonomy_processed.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save list of PIMS that have no text in taxonomy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appending indicators...\n",
      "done!\n",
      "______________________________\n",
      "\n",
      "extracting and merging all text fields - descriptions, objectives, outcomes, outputs, indicators...\n",
      "done!\n",
      "______________________________\n",
      "\n",
      "(655, 98)\n",
      "keep only projects that are currently under implementation...\n",
      "(655, 98)\n",
      "done!\n",
      "______________________________\n",
      "\n",
      "only taxonomy data is considered\n",
      "flag projects that were tagged by looking at project document...\n",
      "number of documents with and without proper text and only titles False         490\n",
      "True          165\n",
      "right_only      0\n",
      "Name: title_only, dtype: int64\n",
      "done!\n",
      "______________________________\n",
      "\n",
      "Checking list of PIMS_ID that are not implemented or cancelled and remove from dataframa if no text exists...\n",
      "Number of projects that will be removed: 0\n",
      "______________________________\n",
      "\n",
      "processing done!\n",
      "final shape of data: (655, 60)\n",
      "deleting all empty fields and creating subset for: all_text\n",
      "___________\n",
      "original data size is: 655\n",
      "remaining projects with non empty field all_text (490, 60)\n",
      "_________________________\n",
      "average lenght of texts: 2702.2442748091603\n",
      "Median of texts: 2595.0\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "(655, 60)\n",
      "(490, 60)\n"
     ]
    }
   ],
   "source": [
    "taxonomy_no_pims, pims = mk.create_training_texts(taxonomy, compare_with_API = False, under_implementation = True)\n",
    "taxonomy_filtered_no_pims, data = mk.create_subset(taxonomy_no_pims, \"all_text\")\n",
    "print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n",
    "print(taxonomy_no_pims.shape)\n",
    "print(taxonomy_filtered_no_pims.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxonomy_no_pims['all_text'] = taxonomy_no_pims['all_text'].fillna('').astype(str)\n",
    "# df1 =  taxonomy_no_pims[taxonomy_no_pims['all_text'].str.contains('[A-Za-z]')]\n",
    "\n",
    "# total_merge = taxonomy_no_pims.merge(df1, on='PIMS_ID', how='outer', indicator=True)\n",
    "# total_merge['all_text_y'] = total_merge['all_text_y'].fillna('').astype(str)\n",
    "\n",
    "# R_1 = total_merge[total_merge['_merge']=='both']\n",
    "# R_2 = total_merge[total_merge['_merge']=='left_only']\n",
    "# R_3 = total_merge[total_merge['_merge']=='right_only']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  detect missing and faulty logframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##save flaged columns and texts:\n",
    "# flag = taxonomy_processed[['PIMS_ID', 'all_text', 'description', 'pims_description', 'pims_objectives','pims_outcomes', 'pims_text','title_only']]\n",
    "# flag = flag.loc[flag['title_only'] == True]\n",
    "# flag = flag.reset_index(drop=True)\n",
    "# flag = flag[['PIMS_ID','pims_text', 'title_only']]\n",
    "# flag['pims_text'] = flag['pims_text'].fillna('not available')\n",
    "\n",
    "# flag = flag.rename(columns={\"title_only\": \"available_from_pims\"})\n",
    "\n",
    "# for index, row in flag.iterrows():\n",
    "#     if row['pims_text'] == 'not available':\n",
    "#         flag.at[index, 'available_from_pims'] = False\n",
    "        \n",
    "#     if len(row['pims_text']) < 600 and row['available_from_pims'] == True:\n",
    "#         flag.at[index, 'available_but_incomplete'] = True\n",
    "#     else:\n",
    "#         flag.at[index, 'available_but_incomplete'] = False\n",
    "           \n",
    "# flag.to_excel(os.path.abspath(os.path.join('..', 'data/temp'))+'/missing_logframes.xlsx')    \n",
    "\n",
    "# # not available or if available non useable list:\n",
    "# flag_2 = flag\n",
    "# for index, row in flag_2.iterrows():\n",
    "    \n",
    "#     if row['available_from_pims'] == True:\n",
    "#         if row['available_but_incomplete'] == False:\n",
    "#             flag_2.drop(index, inplace=True)\n",
    "# flag_2 = flag_2.reset_index(drop=True) \n",
    "           \n",
    "# flag_2.to_excel(os.path.abspath(os.path.join('..', 'data/temp'))+'/checkup.xlsx') \n",
    "\n",
    "# #no logs\n",
    "# no_logs  = flag[flag.available_from_pims == False]\n",
    "# no_logs = no_logs.reset_index(drop=True)\n",
    "\n",
    "# no_logs = no_logs[['PIMS_ID', 'pims_text', 'available_from_pims']]\n",
    "\n",
    "# no_logs.to_excel(os.path.abspath(os.path.join('..', 'data/temp'))+'/no_logframes.xlsx')\n",
    "\n",
    "# #logs but seems incomplete:\n",
    "# incomplete  = flag[flag.available_but_incomplete == True]\n",
    "# incomplete = incomplete.reset_index(drop=True)\n",
    "\n",
    "# incomplete = incomplete[['PIMS_ID', 'pims_text', 'available_but_incomplete']]\n",
    "\n",
    "# incomplete.to_excel(os.path.abspath(os.path.join('..', 'data/temp'))+'/incomplete.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focus on most commonly appearing text: 'all_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting all empty fields and creating subset for: all_text\n",
      "___________\n",
      "original data size is: 635\n",
      "remaining projects with non empty field all_text (604, 65)\n",
      "_________________________\n",
      "average lenght of texts: 4382.806299212599\n",
      "Median of texts: 3473.0\n",
      "save data...\n"
     ]
    }
   ],
   "source": [
    "taxonomy_filtered, data = mk.create_subset(taxonomy_processed, \"all_text\")\n",
    "\n",
    "print('save data...')\n",
    "taxonomy_filtered.to_excel(os.path.abspath(os.path.join('..', 'data/interim'))+'/taxonomy_filtered.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check lenght of text columns for training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3ed54906c346>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mall_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_subset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtaxonomy_filtered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"all_text\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mall_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'all_text_len'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'all_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "all_text, data = mk.create_subset(taxonomy_filtered, \"all_text\")\n",
    "all_text['all_text_len'] = all_text['all_text'].apply(len)\n",
    "print('')\n",
    "print('')\n",
    "logframe, data = mk.create_subset(taxonomy_filtered, \"logframe\")\n",
    "logframe['logframe_len'] = logframe['logframe'].apply(len) \n",
    "print('')\n",
    "print('')\n",
    "description, data = mk.create_subset(taxonomy_filtered, \"description\")\n",
    "description['description_len'] = description['description'].apply(len)\n",
    "print('')\n",
    "print('')\n",
    "objectives, data = mk.create_subset(taxonomy_filtered, \"objectives\")\n",
    "objectives['objectives_len'] = objectives['objectives'].apply(len)\n",
    "print('')\n",
    "print('')\n",
    "outcomes, data = mk.create_subset(taxonomy_filtered, \"outcomes\")\n",
    "outcomes['outcomes_len'] = objectives['outcomes'].apply(len)\n",
    "print('')\n",
    "print('')\n",
    "outputs, data = mk.create_subset(taxonomy_filtered, \"outputs\")\n",
    "outputs['outcomes_len'] = outputs['outputs'].apply(len)\n",
    "print('')\n",
    "print('')\n",
    "indicator, data = mk.create_subset(taxonomy_filtered, \"Indicator\")\n",
    "indicator['indicator_len'] = outputs['Indicator'].apply(len)\n",
    "print('')\n",
    "print('')\n",
    "pims_text, data = mk.create_subset(taxonomy_filtered, \"pims_text\")\n",
    "pims_text['pims_text_len'] = pims_text['pims_text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### description:\n",
    "all_text.hist(column='all_text_len', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn2\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "a = all_text.PIMS_ID.to_list()\n",
    "b = logframe.PIMS_ID.to_list()\n",
    "c = description.PIMS_ID.to_list()\n",
    "d = objectives.PIMS_ID.to_list()\n",
    "e = outputs.PIMS_ID.to_list()\n",
    "f = outcomes.PIMS_ID.to_list()\n",
    "\n",
    "set1 = set(a)\n",
    "set2 = set(b)\n",
    "set3 = set(c)\n",
    "set4 = set(d)\n",
    "set5 = set(e)\n",
    "set6 = set(f)\n",
    "\n",
    "v = venn2([set1, set6], ('all_text', 'outcomes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_only = pd.DataFrame(taxonomy_filtered, columns=[ 'technical_team', 'cross-cutting_programme_1', 'technical_area_1',\n",
    "       'technical_sub-area_1', 'cross-cutting_programme_2', 'technical_area_2',\n",
    "       'technical_sub-area_2', 'transformed_sectors',\n",
    "       'landscapes_1', 'sub-landscapes_1', 'landscapes_2', 'sub-landscapes_2',\n",
    "       'landscapes_3', 'sub-landscapes_3', 'sdg_goal_reference_from_pims+',\n",
    "       'sdg_goal', 'sdg_targets_(top_three)', 'sdg_goal_summary', 'undp_roles',\n",
    "       'main_theme_', 'sub-_strategy_1', 'main_strategy_2', 'sub-_strategy_2',\n",
    "       'main_strategy_3', 'sub-_strategy_3', 'pathways', 'targeted_risks',\n",
    "       '*conventions/protocols/plans', '*social_inclusion_&_engagement',\n",
    "       '*gender_equality', '*types_of_private_sector', '*hot_topics'])\n",
    "\n",
    "multi_class = pd.DataFrame(taxonomy_filtered, columns=['technical_team'])\n",
    "\n",
    "multi_label_upper_3 = pd.DataFrame(taxonomy_filtered, columns=['undp_roles', 'pathways',\n",
    "       'targeted_risks', '*conventions/protocols/plans',\n",
    "       '*social_inclusion_&_engagement', '*gender_equality',\n",
    "       '*types_of_private_sector', '*hot_topics'])\n",
    "\n",
    "\n",
    "sequential_multi_class_multi_label = pd.DataFrame(taxonomy_filtered, columns=['main_strategy_1',  'main_strategy_2', 'main_strategy_3',  'landscapes_1','landscapes_2', 'landscapes_3']) \n",
    "\n",
    "multi_label_hierarchical_upper_2 = pd.DataFrame(taxonomy_filtered, columns=['technical_sub-area_1', 'technical_sub-area_2']) \n",
    "\n",
    "multi_label_hierarchical_upper_3 = pd.DataFrame(taxonomy_filtered, columns=['technical_sub-sub_area_1', 'technical_sub-sub_area_2', \n",
    "                                                                              'sub-landscapes', 'sub-landscapes_2', 'sub-landscapes_3',  \n",
    "                                                                             'sub-_strategy_1', 'sub-_strategy_2', 'sub-_strategy_3']) \n",
    "\n",
    "multi_label_few_hierarchical = pd.DataFrame(taxonomy_filtered, columns=['transformed_sectors']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in taxonomy_filtered.iterrows():\n",
    "    \n",
    "    if row['technical_team'] == 'Climate_Change_Mitigation_Programme':\n",
    "        print(row['PIMS_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Categorisation: Categorical charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cate in multi_label_upper_3:\n",
    "    class_ = vis.disp_category(cate, taxonomy_filtered, min_items=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential multi label/multi class: Categorical charts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cate in sequential_multi_class_multi_label:\n",
    "    print (cate)\n",
    "    class_ = vis.plotly_pie(cate, taxonomy_filtered, min_items=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cate in multi_class:\n",
    "    print (cate)\n",
    "    class_ = vis.plotly_radar(cate, taxonomy_filtered, min_items=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "* Very small training data set.\n",
    "* Highly skewed distribution of training data set. \n",
    "* Highly unbalanced classification problems. \n",
    "* Large number of labels to predict.\n",
    "* Hierarchical structures.\n",
    "* Additional rules - \"only up to two/three categories\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
